{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Supervised Hidden Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "# raw_text = \"\"\"Hươu là loài vật được con_người thuần_dưỡng đã hàng trăm năm .\n",
    "# Nhưng cũng là loài vật nhút_nhát , hễ có tiếng_động là lập_tức tập_trung thính_lực vào đôi tai .\n",
    "# Tai hươu có_thể quay 4 hướng .\n",
    "# Hươu sẵn_sàng tự_vệ bằng cách bỏ chạy khi nghe có tiếng_động lạ .\"\"\"\n",
    "#states: start word is 1, non start word is 2\n",
    "def sentence_to_words_and_states(s):\n",
    "  words = []\n",
    "  states = []\n",
    "  for raw_word in s.split():\n",
    "    if not \"_\" in raw_word:\n",
    "      words.append(raw_word)\n",
    "#       if not raw_word in list(string.punctuation):       \n",
    "#         states.append(1)\n",
    "#       else:\n",
    "#         states.append(3)\n",
    "      states.append(1)\n",
    "    else:\n",
    "      syllables = raw_word.split(\"_\")\n",
    "      words = words + syllables\n",
    "      states.append(2)\n",
    "      for i in range(0, len(syllables) - 1):\n",
    "        states.append(3)\n",
    "  assert(len(words) == len(states))\n",
    "  return words, states\n",
    "\n",
    "def preprocess_sentence(raw_text):\n",
    "  raw_sentences = raw_text.split(\".\")[:-1]\n",
    "  sentences = [sentence for sentence in raw_sentences if len(sentence.strip()) > 0]\n",
    "  lower_sentences = [s.lower() for s in sentences]\n",
    "  return lower_sentences\n",
    "\n",
    "def prepare_sentence_words_and_states(raw_text):\n",
    "  sentences = preprocess_sentence(raw_text)\n",
    "  sentence_words = []\n",
    "  sentence_states = []\n",
    "  for s in sentences:\n",
    "    words, states = sentence_to_words_and_states(s)\n",
    "    sentence_words.append(words)\n",
    "    sentence_states.append(states)\n",
    "  return sentences, sentence_words, sentence_states\n",
    "\n",
    "def prepare_sentence_words(raw_text):\n",
    "  sentences = preprocess_sentence(raw_text)\n",
    "  for s in sentences:\n",
    "    words, _ = sentence_to_words_and_states(s)\n",
    "    sentence_words.append(words)\n",
    "  return sentence_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hươu là loài vật được con_người thuần_dưỡng đã hàng trăm năm \n",
      "[1, 1, 1, 1, 1, 2, 3, 2, 3, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def read_raw_text(dataset_path, output_folder):\n",
    "  output_path = os.path.join(dataset_path, output_folder)\n",
    "  file_paths = [os.path.join(output_path, file_name) for file_name in os.listdir(output_path)]\n",
    "  raw_text = \"\"\n",
    "  for file_path in file_paths:\n",
    "    f = open(file_path, 'r')\n",
    "    raw_text += f.read()\n",
    "    f.close()\n",
    "  return raw_text\n",
    "raw_text = read_raw_text(\"datasets\", \"output\") \n",
    "sentences, sentence_words, sentence_states = prepare_sentence_words_and_states(raw_text)\n",
    "# sentence_words = prepare_sentence_words(raw_text)\n",
    "# sentence_words[1]\n",
    "# for i in range(10, 16):\n",
    "#   print(sentences[i])\n",
    "#   print(sentence_states[i])\n",
    "#   print('\\n')\n",
    "#   assert(len(sentence_words[i]) == len(sentence_states[i]))\n",
    "print(sentences[0])\n",
    "print(sentence_states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_words = 3500\n",
    "n_states = 3\n",
    "only_word_state = [1]\n",
    "start_word_state = [2]\n",
    "in_word_state = [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def build_dataset(sentences, n_words):\n",
    "  words = [word for sentence in sentences for word in sentence]\n",
    "  count = [[\"UNK\", -1]]\n",
    "  count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = []\n",
    "  unk_count = 0\n",
    "  for sentence in sentences:\n",
    "    index_sentence = []\n",
    "    for word in sentence:\n",
    "      index = dictionary.get(word, 0)\n",
    "      if index == 0:\n",
    "        unk_count += 1\n",
    "      index_sentence.append(index)\n",
    "    data.append(index_sentence)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "  return count, data, dictionary, reverse_dictionary\n",
    "count, data, dictionary, reverse_dictionary = build_dataset(sentence_words, n_words)\n",
    "\n",
    "data = np.asarray(data)\n",
    "sentence_states = np.asarray(sentence_states)\n",
    "sentences = np.asarray(sentences)\n",
    "random_indices = np.random.permutation(len(data))\n",
    "data = data[random_indices]\n",
    "sentence_states = sentence_states[random_indices]\n",
    "sentences = sentences[random_indices]\n",
    "test_size = 500\n",
    "X_train = data[:-test_size]\n",
    "y_train = sentence_states[:-test_size]\n",
    "X_val = data[-test_size:]\n",
    "y_val = sentence_states[-test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_obj(obj, name):\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "save_obj(dictionary, 'dictionary')\n",
    "save_obj(reverse_dictionary, 'reverse_dictionary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_probability_matrix(data, sentence_states):\n",
    "  # A[b,a] represents P(y^j = b|y^(j-1) = a), with first row and first column represents the Start state\n",
    "  # O[w,a] represents P(x^j = w|y^j = a), with first column represents the Start state\n",
    "  A = np.zeros((n_states+1, n_states+1))\n",
    "  O = np.zeros((n_words, n_states+1))\n",
    "  for sentence_num in range(len(data)):\n",
    "    for word_num in range(0, len(data[sentence_num])):\n",
    "      previous_state = 0 if word_num == 0 else sentence_states[sentence_num][word_num - 1]\n",
    "      A[sentence_states[sentence_num][word_num], previous_state] += 1\n",
    "  A = A / np.sum(A, axis = 0, keepdims=True)\n",
    "\n",
    "  for sentence_num in range(len(data)):\n",
    "    for word_num in range(0, len(data[sentence_num])):\n",
    "      O[data[sentence_num][word_num], sentence_states[sentence_num][word_num]] += 1\n",
    "  O[:,1:] = O[:,1:] / np.sum(O[:,1:], axis = 0, keepdims=True)\n",
    "  return A, O\n",
    "A, O = build_probability_matrix(data, sentence_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vertibi algorithm for inference\n",
    "def find_best_sequence_markov(A, O, sentence_encoded):\n",
    "  M = len(sentence_encoded)\n",
    "  # best_score[a][b] is the best log probability of sequence ending at position = b having state = a\n",
    "  # best_state[a][b] is the state (from 0) of the position before b that gives max log probability at position b\n",
    "  best_score = np.zeros((n_states, M))\n",
    "  best_state = np.zeros((n_states, M)).astype(int)\n",
    "  for current_word_index in range(M):\n",
    "    with np.errstate(divide='ignore'):\n",
    "      if current_word_index == 0:\n",
    "        best_score[:, 0] = [np.log(A[state][0]) + np.log(O[sentence_encoded[0]][state])\n",
    "                               for state in range(1, n_states + 1)]\n",
    "      else:\n",
    "        for current_state in range(1, n_states + 1):\n",
    "          current_state_log_score = [np.log(A[current_state][previous_state]) + \n",
    "                                     np.log(O[sentence_encoded[current_word_index]][current_state])\n",
    "                                     for previous_state in range(1, n_states + 1)]\n",
    "          current_state_score = best_score[:, current_word_index - 1] + current_state_log_score\n",
    "          best_score[current_state - 1, current_word_index] = np.max(current_state_score)\n",
    "          best_state[current_state - 1, current_word_index] = np.argmax(current_state_score)\n",
    "  # backtrack to find best_sequence\n",
    "  best_sequence = []\n",
    "  current_best_state = np.argmax(best_score[:, M - 1])\n",
    "  best_sequence = [current_best_state + 1]\n",
    "  #print(best_state)\n",
    "  for current_word_index in range(M - 1, 0, -1):\n",
    "    #print(\"current_best_state: \", current_best_state, \"current_word_index: \", current_word_index)\n",
    "    previous_best_state = best_state[current_best_state, current_word_index]\n",
    "    best_sequence = [previous_best_state + 1] + best_sequence\n",
    "    current_best_state = previous_best_state\n",
    "  return best_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def print_word_in_sentence(sentence_encoded, sentence_state, reverse_dictionary):\n",
    "  s = \"\"\n",
    "  M = len(sentence_encoded)\n",
    "  for word_index in range(M):\n",
    "    current_word = reverse_dictionary[sentence_encoded[word_index]]\n",
    "    if sentence_state[word_index] in only_word_state:\n",
    "      if word_index < M - 1:\n",
    "        s += current_word + \" \"\n",
    "      else:\n",
    "        s += current_word\n",
    "    elif sentence_state[word_index] in start_word_state:\n",
    "      if word_index < M - 1 and sentence_state[word_index + 1] in in_word_state:\n",
    "        s += current_word + \"_\"\n",
    "      elif word_index < M - 1:\n",
    "        s += current_word + \" \"\n",
    "      else:\n",
    "        s += current_word\n",
    "    else:\n",
    "      if word_index < M - 1 and sentence_state[word_index + 1] in in_word_state:\n",
    "        s += current_word + \"_\"\n",
    "      elif word_index < M - 1:\n",
    "        s += current_word + \" \"\n",
    "      else:\n",
    "        s += current_word\n",
    "  return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II Conditional Random Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create transition feature and matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "n_words, n_states = 3500, 3\n",
    "# Input: yj_prev: previous state, yj: current state, xj: current word\n",
    "# Output: vector phi representing the transition from previous state to current state given current word\n",
    "# n_states does not include Start\n",
    "def create_transition_feature(yj_prev, yj, xj):\n",
    "  assert(yj_prev <= n_states and yj <= n_states and xj < n_words)\n",
    "  phi_1 = np.zeros((n_states * n_words, 1))\n",
    "  phi_2 = np.zeros(((n_states + 1) * n_states, 1))\n",
    "  phi_1[(yj - 1) * n_words + xj, 0] = 1\n",
    "  phi_2[(yj - 1) * (n_states + 1) + yj_prev, 0] = 1\n",
    "  phi = np.concatenate((phi_1, phi_2), axis = 0)\n",
    "  return phi\n",
    "\n",
    "# def create_transition_feature_2(yj_prev, yj, xj):\n",
    "#   assert(yj_prev <= n_states and yj <= n_states and xj < n_words)\n",
    "#   row = np.array([(yj - 1) * n_words + xj, n_states * n_words + (yj - 1) * (n_states + 1) + yj_prev])\n",
    "#   col = np.array([0, 0])\n",
    "#   data = np.array([1, 1])\n",
    "#   return csr_matrix((data, (row, col)), shape=(n_states * n_words + (n_states + 1) * n_states, 1))\n",
    "\n",
    "def transition_position(yj_prev, yj, xj):\n",
    "  return ((yj - 1) * n_words + xj, 0), (n_states * n_words + (yj - 1) * (n_states + 1) + yj_prev, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute numerator F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature function a position j\n",
    "def feature_function(w, theta, yj, yj_prev, xj, xj_prev):\n",
    "  f = 0\n",
    "  p1, p2 = transition_position(yj_prev, yj, xj)\n",
    "  f += w[p1] + w[p2]\n",
    "  if xj_prev != -1:\n",
    "    sign = 1 if yj in in_word_state else -1\n",
    "    f += sign * np.dot(theta[xj, :], theta[xj_prev, :])\n",
    "  return f\n",
    "    \n",
    "def compute_F(w, theta, y, x):\n",
    "  f_sum = 0\n",
    "  for j in range(len(x)):\n",
    "    yj_prev = 0 if j == 0 else y[j - 1]\n",
    "    xj_prev = -1 if j == 0 else x[j - 1]\n",
    "    f_sum += feature_function(w, theta, y[j], yj_prev, x[j], xj_prev)\n",
    "  return f_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute normalizer Z by backtrack and dynamic programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_log_Z_backtrack_step(w, theta, y, x, k):\n",
    "  if k == -1:\n",
    "    assert (len(y) == len(x)) \n",
    "    return 1\n",
    "  else:\n",
    "    sum_Z = 0\n",
    "    for yj in range(1, n_states + 1):\n",
    "      y = [yj] + y\n",
    "      sum_Z += compute_log_Z_backtrack_step(w, theta, y, x, k - 1)\n",
    "      y = y[1:]\n",
    "  return sum_Z\n",
    "    \n",
    "      \n",
    "def compute_log_Z_backtrack(w, theta, x):\n",
    "  #return np.log(compute_log_Z_backtrack_step(w, theta, [], x, len(x) - 1))\n",
    "  return compute_log_Z_backtrack_step(w, theta, [], x, len(x) - 1)\n",
    "\n",
    "def compute_log_Z(w, theta, x):\n",
    "  M = len(x)\n",
    "  G = np.zeros((n_states, 1))\n",
    "  sum_log_C = 0\n",
    "  \n",
    "  # case j = 0 (first word of x)\n",
    "  for yj in range(1, n_states + 1):\n",
    "    G[yj - 1, 0] = np.exp(feature_function(w, theta, yj, 0, x[0], -1))\n",
    "  C = np.sum(G)\n",
    "  G /= C\n",
    "  sum_log_C += np.log(C)\n",
    "    \n",
    "  for j in range(1, M):\n",
    "    G_j = np.zeros((n_states, n_states))\n",
    "    for yj_prev in range(1, n_states + 1):\n",
    "      for yj in range(1, n_states + 1):\n",
    "        G_j[yj - 1, yj_prev - 1] = np.exp(feature_function(w, theta, yj, yj_prev, x[j], x[j - 1]))\n",
    "    G = np.matmul(G_j, G)\n",
    "    C = np.sum(G)\n",
    "    G /= C\n",
    "    sum_log_C += np.log(C)\n",
    "  return np.log(np.sum(G)) + sum_log_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 8\n",
    "embedded_size = 2\n",
    "w = np.random.rand(n_states * n_words + (n_states + 1) * n_states, 1)\n",
    "theta = np.random.rand(n_words, embedded_size)\n",
    "assert(compute_log_Z(w, theta, data[i]) - compute_log_Z_backtrack(w, theta, data[i]) < 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_log_conditional_prob(w, theta, y, x):\n",
    "  return compute_F(w, theta, y, x) - compute_log_Z(w, theta, x)\n",
    "\n",
    "def compute_minus_log_conditional_prob(w, theta, y, x):\n",
    "  return -compute_log_conditional_prob(w, theta, y, x)\n",
    "\n",
    "def compute_minus_log_conditional_prob_batch(w, theta, y_batches, x_batches):\n",
    "  batch_prob = 0\n",
    "  for y, x in zip(y_batches, x_batches):\n",
    "    batch_prob += compute_minus_log_conditional_prob(w, theta, y, x)\n",
    "  return batch_prob\n",
    "\n",
    "def cost_regularized(w, theta, y_batches, x_batches, lambda_w, lambda_theta):\n",
    "  return lambda_w * np.dot(w.T, w)[0,0] + lambda_theta * np.sum(theta * theta) + \\\n",
    "         compute_minus_log_conditional_prob_batch(w, theta, y_batches, x_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerical_grad(w, theta, cost, *args):\n",
    "  grad_w = np.zeros_like(w)\n",
    "  grad_theta = np.zeros_like(theta)\n",
    "  eps = 1e-4\n",
    "  for i in range(len(w)):\n",
    "    w_high = w.copy()\n",
    "    w_low = w.copy()\n",
    "    w_high[i] += eps\n",
    "    w_low[i] -= eps\n",
    "    grad_w[i] = (cost(w_high, theta, *args) - cost(w_low, theta, *args)) / (2 * eps)\n",
    "  for i in range(theta.shape[0]):\n",
    "    for j in range(theta.shape[1]):\n",
    "      theta_high = theta.copy()\n",
    "      theta_low = theta.copy()\n",
    "      theta_high[i, j] += eps\n",
    "      theta_low[i, j] -= eps\n",
    "      grad_theta[i, j] = (cost(w, theta_high, *args) - cost(w, theta_low, *args)) / (2 * eps)\n",
    "  return grad_w, grad_theta\n",
    "\n",
    "def check_grad(w, theta, cost, grad, *args):\n",
    "  num_grad_w, num_grad_theta = numerical_grad(w, theta, cost,  *args)\n",
    "  real_grad_w, real_grad_theta = grad(w, theta, *args)\n",
    "  if np.linalg.norm(real_grad_w - num_grad_w) < 1e-6  and np.linalg.norm(real_grad_theta - num_grad_theta) < 1e-6:\n",
    "    return True\n",
    "  return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute gradient wrt F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_gradient_wrt_F(w, theta, y, x):\n",
    "  grad_w = np.zeros((n_states * n_words + (n_states + 1) * n_states, 1))\n",
    "  grad_theta = np.zeros((n_words, embedded_size))\n",
    "  for j in range(len(x)):\n",
    "    y_prev = 0 if j == 0 else y[j - 1]\n",
    "    p1, p2 = transition_position(y_prev, y[j], x[j])\n",
    "    grad_w[p1] += 1\n",
    "    grad_w[p2] += 1\n",
    "    if j > 0:\n",
    "      sign = 1 if y[j] in in_word_state else -1\n",
    "      grad_theta[x[j], :] += sign * theta[x[j - 1], :]\n",
    "      grad_theta[x[j - 1], :] += sign * theta[x[j], :]\n",
    "  return grad_w, grad_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6618f905e6db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_F\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_gradient_wrt_F\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'w' is not defined"
     ]
    }
   ],
   "source": [
    "embedded_size = 5\n",
    "theta = np.random.rand(n_words, embedded_size)\n",
    "y = sentence_states[6]\n",
    "x = data[6]\n",
    "assert(check_grad(w, theta, compute_F, compute_gradient_wrt_F, y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute gradient wrt to the normalizer Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input: parameters w and encoded sentence x\n",
    "# Output:\n",
    "  # G runs from the transition 1 -> 2 to M-1 -> M (position not index in array)\n",
    "  # alpha runs from j = 1 to j = M-1. For example, alpha[0][0] is the sum of prob of all length 1 sequence \n",
    "    # having the state = 0 (1 - 1)\n",
    "  # beta runs from j = 2 to j = M. For example, beta[0][0] is the sum of prob of all length M-1 (from postion \n",
    "    # 2 to M) having the position 2 as state 0 (1 - 1)\n",
    "def forward_backward(w, theta, x):\n",
    "  alpha = np.zeros((n_states, 1))\n",
    "  M = len(x)\n",
    "  # case j = 0 (first word of x)\n",
    "  for yj in range(1, n_states + 1):\n",
    "    alpha[yj - 1, 0] = np.exp(feature_function(w, theta, yj, 0, x[0], -1))\n",
    "  C = np.sum(alpha)\n",
    "  alpha /= C\n",
    "  alpha_list = [alpha]\n",
    "  beta = np.ones((n_states, 1))\n",
    "  beta_list = [beta]\n",
    "  G_list = []\n",
    "  \n",
    "  # No need to calculate G and beta for length 1 string \n",
    "  if M == 1:\n",
    "    alpha = np.concatenate(alpha_list, axis = 1)\n",
    "    return alpha, [], []\n",
    "  for j in range(1, M):\n",
    "    G_j = np.zeros((n_states, n_states))\n",
    "    for yj_prev in range(1, n_states + 1):\n",
    "      for yj in range(1, n_states + 1):\n",
    "        G_j[yj - 1, yj_prev - 1] = feature_function(w, theta, yj, yj_prev, x[j], x[j - 1])\n",
    "    G_j = np.exp(G_j)\n",
    "    G_list.append(G_j)\n",
    "    if j < M - 1:\n",
    "      alpha = np.matmul(G_j, alpha)\n",
    "      C = np.sum(alpha)\n",
    "      alpha /= C\n",
    "      alpha_list.append(alpha)\n",
    "  \n",
    "  alpha = np.concatenate(alpha_list, axis = 1)\n",
    "  G = np.stack(G_list)\n",
    "  \n",
    "  for j in range(M - 2, 0, -1):\n",
    "    beta = np.matmul(G[j].T, beta)\n",
    "    C = np.sum(beta)\n",
    "    beta /= C\n",
    "    beta_list = [beta] + beta_list\n",
    "  beta = np.concatenate(beta_list, axis = 1)\n",
    "  assert(len(alpha_list) == M - 1)\n",
    "  assert(len(beta_list) == M - 1)\n",
    "  assert(len(G_list) == M - 1)\n",
    "  return alpha, beta, G\n",
    "\n",
    "def compute_gradient_wrt_log_Z(w, theta, x):\n",
    "  alpha, beta, G = forward_backward(w, theta, x)\n",
    "  M = len(x)\n",
    "  grad_w = np.zeros((n_states * n_words + (n_states + 1) * n_states, 1))\n",
    "  grad_theta = np.zeros((n_words, embedded_size))\n",
    "  # handle for the first position, need to find the sum of prob for each state in the first position\n",
    "  if M == 1: # for length 1 string\n",
    "    first_position_prob = alpha[:, 0]\n",
    "  else:\n",
    "    first_position_prob = np.multiply(alpha[:, 0], np.matmul(G[0].T, beta[:, 0]))\n",
    "  first_position_prob /= np.sum(first_position_prob)\n",
    "  for yj in range(1, n_states + 1):\n",
    "      p1, p2 = transition_position(0, yj, x[0])\n",
    "      grad_w[p1] += first_position_prob[yj - 1]\n",
    "      grad_w[p2] += first_position_prob[yj - 1]\n",
    "  if M == 1:\n",
    "    return grad_w, grad_theta\n",
    "  \n",
    "  # handle for position 2 to M\n",
    "  T = np.expand_dims(alpha.T, axis=1) * (G * np.expand_dims(beta.T, axis = 2))\n",
    "  S = np.sum(np.sum(T, axis = 1), axis=1)\n",
    "  S = np.expand_dims(np.expand_dims(S, axis = 1), axis = 1)\n",
    "  T = T / S  \n",
    "  \n",
    "  for j in range(1, M): \n",
    "    for yj_prev in range(1, n_states + 1):\n",
    "      for yj in range(1, n_states + 1):\n",
    "        #prob_yj_prev_and_yj = alpha[:, j - 1][yj_prev - 1]*G[j-1][yj - 1, yj_prev - 1]*beta[:, j - 1][yj - 1]\n",
    "        prob_yj_prev_and_yj = T[j - 1, yj - 1, yj_prev - 1]\n",
    "        p1, p2 = transition_position(yj_prev, yj, x[j])\n",
    "        grad_w[p1] += prob_yj_prev_and_yj\n",
    "        grad_w[p2] += prob_yj_prev_and_yj\n",
    "        sign = 1 if yj in in_word_state else -1\n",
    "        grad_theta[x[j], :] += sign * prob_yj_prev_and_yj * theta[x[j - 1], :]\n",
    "        grad_theta[x[j - 1], :] += sign * prob_yj_prev_and_yj * theta[x[j], :]\n",
    "  return grad_w, grad_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 19\n",
    "embedded_size = 2\n",
    "w = np.random.rand(n_states * n_words + (n_states + 1) * n_states, 1)\n",
    "theta = np.random.rand(n_words, embedded_size)\n",
    "x = data[i]\n",
    "num_grad_w, num_grad_theta = numerical_grad(w, theta, compute_log_Z, data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01889204978942871\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "grad_w, grad_theta = compute_gradient_wrt_log_Z(w, theta, x)\n",
    "assert(np.linalg.norm(grad_w - num_grad_w) < 1e-6  and \n",
    "       np.linalg.norm(grad_theta - num_grad_theta) < 1e-6)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute overall gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_gradient_wrt_conditional_prob(w, theta, y, x):\n",
    "  grad_w1, grad_theta1 = compute_gradient_wrt_F(w, theta, y, x)\n",
    "  grad_w2, grad_theta2 = compute_gradient_wrt_log_Z(w, theta, x)\n",
    "  return -grad_w1 + grad_w2, -grad_theta1 + grad_theta2\n",
    "\n",
    "def compute_gradient_wrt_conditional_prob_batch(w, theta, y_batches, x_batches):\n",
    "  grad_w = 0\n",
    "  grad_theta = 0\n",
    "  for y_batch, x_batch in zip(y_batches, x_batches):\n",
    "    grad_w_batch, grad_theta_batch = compute_gradient_wrt_conditional_prob(w, theta, y_batch, x_batch)\n",
    "    grad_w += grad_w_batch\n",
    "    grad_theta += grad_theta_batch\n",
    "  return grad_w, grad_theta\n",
    "\n",
    "def grad_regularized(w, theta, y_batches, x_batches, lambda_w, lambda_theta):\n",
    "  grad_w, grad_theta = compute_gradient_wrt_conditional_prob_batch(w, theta, y_batches, x_batches)\n",
    "  return 2 * lambda_w * w + grad_w, 2 * lambda_theta * theta + grad_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Check gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 19\n",
    "embedded_size = 2\n",
    "w = np.random.rand(n_states * n_words + (n_states + 1) * n_states, 1)\n",
    "theta = np.random.rand(n_words, embedded_size)\n",
    "y_batches = sentence_states[i:i+1]\n",
    "x_batches = data[i:i+1]\n",
    "num_grad_w, num_grad_theta = numerical_grad(w, theta, cost_regularized, y_batches, x_batches, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "grad_w, grad_theta = grad_regularized(w, theta, y_batches, x_batches, 0.5, 0.5)\n",
    "assert(np.linalg.norm(grad_w - num_grad_w) < 1e-6)\n",
    "assert(np.linalg.norm(grad_theta - num_grad_theta) < 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6771039962768555\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "grad_regularized(w, theta, sentence_states[:100], data[:100], 0.5, 0.5)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(500)\n",
    "def generate_batch(X_train, y_train, n_batches):\n",
    "  random_indices = np.random.permutation(len(X_train))\n",
    "  X_train = X_train[random_indices]\n",
    "  y_train = y_train[random_indices]\n",
    "  X_batches = np.array_split(X_train, n_batches)\n",
    "  y_batches = np.array_split(y_train, n_batches)\n",
    "  return X_batches, y_batches \n",
    "\n",
    "eta = 0.001\n",
    "n_epochs = 31\n",
    "batch_size = 128\n",
    "n_batches = len(X_train) // batch_size\n",
    "beta1 = 0.9 \n",
    "beta2 = 0.999\n",
    "epsilon = 1e-8\n",
    "lambda_w = 0.5\n",
    "lambda_theta = 0.5\n",
    "\n",
    "def crfGradientDescent(w_init, theta_init, cost, grad, adam = False):\n",
    "  w = w_init\n",
    "  theta = theta_init\n",
    "  v_dw = 0\n",
    "  s_dw = 0\n",
    "  v_dtheta = 0\n",
    "  s_dtheta = 0\n",
    "  iteration = 1\n",
    "  count_since_best = 0\n",
    "  best_val_loss = 1e10\n",
    "  best_w = w\n",
    "  best_theta = theta\n",
    "  for epoch in range(n_epochs):\n",
    "    if epoch % 2 == 0:\n",
    "      print(\"Epoch: \", epoch, \" Val cost: \", cost(w, theta, y_val, X_val, 0, 0))\n",
    "    X_batches, y_batches = generate_batch(X_train, y_train, n_batches)\n",
    "    for X_batch, y_batch in zip(X_batches, y_batches):\n",
    "      new_grad_w, new_grad_theta = grad(w, theta, y_batch, X_batch, lambda_w, lambda_theta)\n",
    "      if adam:\n",
    "        v_dw = beta1*v_dw + (1 - beta1)*new_grad_w\n",
    "        s_dw = beta2*s_dw + (1 - beta2)*new_grad_w*new_grad_w\n",
    "        v_dw_corrected = v_dw / (1 - beta1 ** iteration)\n",
    "        s_dw_corrected = s_dw / (1 - beta2 ** iteration)\n",
    "        w = w - eta * v_dw_corrected / (np.sqrt(s_dw_corrected) + epsilon)\n",
    "        \n",
    "        v_dtheta = beta1*v_dtheta + (1 - beta1)*new_grad_theta\n",
    "        s_dtheta = beta2*s_dtheta + (1 - beta2)*new_grad_theta*new_grad_theta\n",
    "        v_dtheta_corrected = v_dtheta / (1 - beta1 ** iteration)\n",
    "        s_dtheta_corrected = s_dtheta / (1 - beta2 ** iteration)\n",
    "        theta = theta - eta * v_dtheta_corrected / (np.sqrt(s_dtheta_corrected) + epsilon)\n",
    "      else:\n",
    "        w = w - eta*new_grad_w\n",
    "        theta = theta - eta*new_grad_theta\n",
    "      count_since_best += 1\n",
    "      iteration += 1\n",
    "      if iteration % 50 == 0:\n",
    "        val_loss = cost(w, theta, y_val, X_val, 0, 0)\n",
    "        if val_loss < best_val_loss:\n",
    "          best_val_lost = val_loss\n",
    "          count_since_best = 0\n",
    "          best_w = w\n",
    "          best_theta = theta\n",
    "      if count_since_best > 800:\n",
    "        print(\"Early stopping\")\n",
    "        return best_w, best_theta\n",
    "  return w, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Val cost:  2521.28464081\n",
      "Epoch:  2  Val cost:  2540.27870753\n",
      "Epoch:  4  Val cost:  2535.18831715\n",
      "Epoch:  6  Val cost:  2528.06986244\n",
      "Epoch:  8  Val cost:  2520.44799294\n",
      "Epoch:  10  Val cost:  2514.13554136\n",
      "Epoch:  12  Val cost:  2508.6990382\n",
      "Epoch:  14  Val cost:  2502.66826391\n",
      "Epoch:  16  Val cost:  2497.35829254\n",
      "Epoch:  18  Val cost:  2491.20650822\n",
      "Epoch:  20  Val cost:  2487.8266136\n",
      "Epoch:  22  Val cost:  2482.69476205\n",
      "Epoch:  24  Val cost:  2479.92171989\n",
      "Epoch:  26  Val cost:  2477.4600676\n",
      "Epoch:  28  Val cost:  2473.62604565\n",
      "Epoch:  30  Val cost:  2469.69138629\n"
     ]
    }
   ],
   "source": [
    "embedded_size = 15\n",
    "# w_init = np.random.rand(n_states * n_words + (n_states + 1) * n_states, 1)\n",
    "# theta_init = np.random.rand(n_words, embedded_size)\n",
    "w, theta = crfGradientDescent(w, theta, cost_regularized, grad_regularized, adam = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('weight/crf_w.txt', w)\n",
    "np.savetxt('weight/crf_theta.txt', theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468.7346481324739"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_regularized(w, theta, y_val, X_val, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = np.loadtxt('weight/crf_w.txt', dtype=np.float64)\n",
    "theta = np.loadtxt('weight/crf_theta.txt', dtype=np.float64)\n",
    "w = w.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10512, 1)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vertibi algorithm for finding the best sequence\n",
    "# Input:\n",
    "  # w: parameters\n",
    "  # x: encoded sentence\n",
    "# Output:\n",
    "  # sequence of best states\n",
    "def find_best_sequence_crf(w, theta, x):\n",
    "  M = len(x)\n",
    "  G = np.zeros((n_states, 1))\n",
    "  G_max_index = []\n",
    "  # case j = 0 (first word of x)\n",
    "  for yj in range(1, n_states + 1):\n",
    "    G[yj - 1, 0] = feature_function(w, theta, yj, 0, x[0], -1)\n",
    "  \n",
    "  for j in range(1, M):\n",
    "    G_j = np.zeros((n_states, n_states))\n",
    "    for yj_prev in range(1, n_states + 1):\n",
    "      for yj in range(1, n_states + 1):\n",
    "        G_j[yj - 1, yj_prev - 1] = feature_function(w, theta, yj, yj_prev, x[j], x[j - 1])\n",
    "    G = G_j + G.T\n",
    "    G_max_index.append(np.expand_dims(np.argmax(G, axis=1), axis=1))\n",
    "    G = np.max(G, axis = 1, keepdims = True)\n",
    "  if len(G_max_index) > 0:\n",
    "    G_max_index = np.concatenate(G_max_index, axis = 1)\n",
    "    G_max_index_length = G_max_index.shape[1]\n",
    "  else:\n",
    "    G_max_index_length = 0\n",
    "  current_max = np.argmax(G)\n",
    "  best_sequence = [current_max + 1]\n",
    "  for i in range(G_max_index_length - 1, -1, -1):\n",
    "    current_max = G_max_index[current_max, i]\n",
    "    best_sequence = [current_max + 1] + best_sequence\n",
    "  return best_sequence\n",
    "\n",
    "def find_best_sequence_batches(x_val, method = \"\", *args):\n",
    "  if method == \"crf\":\n",
    "    return [find_best_sequence_crf(*args, x) for x in x_val]\n",
    "  elif method == \"markov\":\n",
    "    return [find_best_sequence_markov(*args, x) for x in x_val]\n",
    "  else:\n",
    "    return [np.ones(len(x)) for x in x_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vđv bị truất quyền không được bất_kì giải_thưởng , huy_chương , cúp , xếp_hạng của toàn_bộ giải đấu\n",
      "\n",
      "\n",
      "vđv bị truất quyền không được bất_kì giải_thưởng , huy_chương , cúp , xếp_hạng của toàn_bộ giải_đấu\n",
      "\n",
      "\n",
      "vđv bị truất quyền không được bất_kì giải_thưởng , huy_chương , cúp , xếp_hạng của toàn_bộ giải_đấu\n"
     ]
    }
   ],
   "source": [
    "i = 353\n",
    "#print(data[i])\n",
    "#print(sentence_states[i])\n",
    "#print(find_best_sequence_markov(data[i], A, O))\n",
    "print(print_word_in_sentence(X_val[i], y_val[i], reverse_dictionary))\n",
    "print('\\n')\n",
    "print(print_word_in_sentence(X_val[i], find_best_sequence_crf(w, theta, X_val[i]), reverse_dictionary))\n",
    "print('\\n')\n",
    "print(print_word_in_sentence(X_val[i], find_best_sequence_markov(X_val[i], A, O), reverse_dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i) Scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_sentence(y, y_pred):\n",
    "  assert(len(y) == len(y_pred))\n",
    "  index = 0\n",
    "  TP = 0\n",
    "  n_predicted_words = 0\n",
    "  n_true_words = 0\n",
    "  # find number of correct prediction and total number of predicted words\n",
    "  while index < len(y_pred):\n",
    "    old_index = index\n",
    "    if y_pred[index] in start_word_state or y_pred[index] in only_word_state:  \n",
    "      while index < len(y_pred) - 1 and y_pred[index + 1] in in_word_state:\n",
    "        index += 1\n",
    "    if index > old_index: # case find a multi-syllables word\n",
    "      correct_prediction = y[old_index] in start_word_state\n",
    "      for i in range(1, index - old_index + 1):\n",
    "        if y[old_index + i] != y_pred[old_index + i]:\n",
    "          correct_prediction = False\n",
    "      if index < len(y_pred) - 1 and  y[index + 1] in in_word_state: # the true word is longer\n",
    "        correct_prediction = False\n",
    "    else: # case found a one syllable word\n",
    "      correct_prediction = y[index] in only_word_state\n",
    "    n_predicted_words += 1\n",
    "    TP += correct_prediction\n",
    "    index += 1\n",
    "    \n",
    "  # find total number of true words\n",
    "  index = 0\n",
    "  while index < len(y):\n",
    "    if y[index] in start_word_state:  \n",
    "      while index < len(y) - 1 and y[index + 1] in in_word_state:\n",
    "        index += 1\n",
    "    index += 1\n",
    "    n_true_words += 1\n",
    "  return TP, n_predicted_words, n_true_words\n",
    "#   precision = TP / n_predicted_words\n",
    "#   recall = TP / n_true_words\n",
    "#   eps = 1e-8\n",
    "#   F1 = 2 / (1/(precision + eps) + 1/(recall + eps))\n",
    "#   return precision, recall, F1\n",
    "\n",
    "def score_val(y_val, y_val_pred):\n",
    "  TP, n_predicted_words, n_true_words = 0, 0, 0\n",
    "  for index in range(len(y_val)):\n",
    "    current_TP, current_n_predicted_words, current_n_true_words = score_sentence(y_val[index], y_val_pred[index])\n",
    "    TP += current_TP\n",
    "    n_predicted_words += current_n_predicted_words\n",
    "    n_true_words += current_n_true_words\n",
    "  precision = TP / n_predicted_words\n",
    "  recall = TP / n_true_words\n",
    "  eps = 1e-8\n",
    "  F1 = 2 / (1/(precision + eps) + 1/(recall + eps))\n",
    "  return precision, recall, F1\n",
    "\n",
    "# def score_val2(y_val, y_val_pred):\n",
    "#   precision, recall, F1 = 0, 0, 0\n",
    "#   for index in range(len(y_val)):\n",
    "#     current_precision, current_recall, current_F1 = score_sentence(y_val[index], y_val_pred[index])\n",
    "#     precision += current_precision\n",
    "#     recall += current_recall\n",
    "#     F1 += current_F1\n",
    "#   precision /= len(y_val)\n",
    "#   recall /= len(y_val)\n",
    "#   F1 /= len(y_val)\n",
    "#   print(precision)\n",
    "#   print(recall)\n",
    "#   return precision, recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  2\n",
      "Predicted words:  3\n",
      "True words:  5\n"
     ]
    }
   ],
   "source": [
    "y =      [1, 1, 1, 2, 3, 3, 3, 3, 2, 3]\n",
    "y_pred = [2, 3, 3, 2, 3, 3, 3, 3, 2, 3]\n",
    "TP, n_predicted_words, n_true_words = score_sentence(y, y_pred)\n",
    "print(\"TP: \", TP)\n",
    "print(\"Predicted words: \", n_predicted_words)\n",
    "print(\"True words: \", n_true_words)\n",
    "assert(TP == 2)\n",
    "assert(n_predicted_words == 3)\n",
    "assert(n_true_words == 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "khu_vực doanh_nghiệp ( tư_nhân ) yếu\n",
      "\n",
      "\n",
      "khu_vực doanh_nghiệp ( tư_nhân ) yếu\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "print(print_word_in_sentence(X_val[i], y_val[i], reverse_dictionary))\n",
    "print('\\n')\n",
    "print(print_word_in_sentence(X_val[i], find_best_sequence_crf(w, theta, X_val[i]), reverse_dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii) Conditional random field evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9354320047389354\n",
      "Recall:  0.9487597631104626\n",
      "F1:  0.9420487572307712\n",
      "1.0305149555206299\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "y = y_val\n",
    "y_pred = find_best_sequence_batches(X_val, \"crf\", w, theta)\n",
    "precision, recall, F1 = score_val(y, y_pred)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1: \", F1)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii) Markov hidden model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9151775077436264\n",
      "Recall:  0.9301743622860833\n",
      "F1:  0.922615006597802\n"
     ]
    }
   ],
   "source": [
    "y = y_val\n",
    "y_pred = find_best_sequence_batches(X_val, \"markov\", A, O)\n",
    "precision, recall, F1 = score_val(y, y_pred)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1: \", F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iv) Baseline evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.48573969759857694\n",
      "Recall:  0.6612851146270584\n",
      "F1:  0.5600793183393767\n"
     ]
    }
   ],
   "source": [
    "y = y_val\n",
    "y_pred = find_best_sequence_batches(X_val, \"\")\n",
    "precision, recall, F1 = score_val(y, y_pred)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1: \", F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 1, 0])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([2, 3, 6, 4])\n",
    "(-a).argsort()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
